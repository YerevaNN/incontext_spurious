_target_: src.datamodules.WaterbirdsEmbContextsDataModuleV2
name: waterbirds_emb_contexts

context_class_size: 50
group_proportions: [0.45, 0.05, 0.05, 0.45] # train set default [0.7295098900794983, 0.03837330639362335, 0.011678831651806831, 0.22043795883655548]

root_dir: ${oc.env:DATA_ROOT_DIR}
encoding_extractor: dinov2_vitb14
saved_val_sets_path: null # ${datamodule.root_dir}/waterbirds/context_val_sets_${datamodule.context_class_size}/${datamodule.encoding_extractor}

train_len: ${trainer.max_steps}
eval_len: 1024 # 2^n, because CombinedDataloader removes the last non-full batch

batch_size: 32
num_workers: 4

spurious_setting: ${spurious_setting}  # Use the global value of 'spurious_setting'

v1_behavior: ${v1_behavior}

rotate_encodings: True
n_rotation_matrices: 1000  # 1000 works well, but is slow. Reduce for efficiency when experimenting

randomly_swap_labels: True

val_sets: [train, train_val, val]  # the order here should match the order in the combined loader

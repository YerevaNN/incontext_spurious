_target_: src.datamodules.WaterbirdsEmbContextsDataModuleV2
name: waterbirds_emb_contexts

context_class_size: 50
group_proportions: [0.45, 0.05, 0.05, 0.45] # train set default [0.7295098900794983, 0.03837330639362335, 0.011678831651806831, 0.22043795883655548]

root_dir: ${oc.env:DATA_ROOT_DIR}
encoding_extractor: dinov2_vitb14
saved_val_sets_path: null # ${datamodule.root_dir}/waterbirds/context_val_sets_${datamodule.context_class_size}/${datamodule.encoding_extractor}

train_len: ${trainer.max_steps}
eval_len: 1024 # 2^n, because CombinedDataloader removes the last non-full batch

batch_size: 32
num_workers: 4

spurious_setting: ${spurious_setting}  # Use the global value of 'spurious_setting'
sp_token_generation_mode: ${sp_token_generation_mode}  # Use the global value of 'sp_token_generation_mode'

v1_behavior: ${v1_behavior}

rotate_encodings: False
n_rotation_matrices: 1000  # 1000 might not be enough for waterbirds, because the train size ~= 4k.

randomly_swap_labels: False

label_noise_ratio_interval: null  # [min_ratio, max_ratio], the label noise ratio is uniformly sampled from this interval
input_noise_norm_interval: null  # [min_norm, max_norm], the norm of the Gaussian noise will be uniformly sampled from this interval
permute_input_dim: True

ask_context_prob: null

val_sets: [train, train_val, train_test, val]  # the order here should match the order in the combined loader
